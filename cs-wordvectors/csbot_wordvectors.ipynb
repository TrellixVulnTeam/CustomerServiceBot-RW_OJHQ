{
 "cells": [
  {
   "source": [
    "This notebook creates, trains, and initializes a chatbot using word vectors. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "WARNING:tensorflow:From C:\\Users\\owner\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from os import path, name, system\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from random import choice, randint\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk, pickle, json, re, string, tflearn, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cs_prompts.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and format the training data\n",
    "def preprocess_train_data(data):\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    words = []\n",
    "    labels = list(data.keys())\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for label in labels:\n",
    "        for pattern in data[label]['patterns']:\n",
    "            tokens = nltk.word_tokenize(pattern)\n",
    "            words.extend(tokens)\n",
    "            docs_x.append(tokens)\n",
    "            docs_y.append(label)\n",
    "\n",
    "    # Pass over punctuation tokens\n",
    "    ignored_tokens = [',', '.', '?', '!']\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignored_tokens]\n",
    "\n",
    "    words = sorted(set(words))\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    # Template for the BOW\n",
    "    out_empty = list(np.zeros(len(labels)))\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "        stemmed = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in stemmed:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)  \n",
    "\n",
    "    training = np.array(training)\n",
    "    output = np.array(output)    \n",
    "    \n",
    "    return words, labels, training, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for key in data.keys():\n",
    "    for pattern in data[key]['patterns']:\n",
    "        sentences.append(pattern.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences, size=32, window=3, sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.00454417, -0.00302309,  0.01508945, -0.01440222,  0.00749125,\n",
       "       -0.00686202,  0.01226617, -0.00274484, -0.0072758 , -0.00479306,\n",
       "       -0.01184115,  0.01131314,  0.00633893, -0.01220932,  0.01088379,\n",
       "        0.00815971,  0.01139953,  0.00808345,  0.01503804, -0.01106866,\n",
       "       -0.0108741 , -0.01006933, -0.00208098,  0.00145122,  0.00448123,\n",
       "       -0.00325523,  0.00813822,  0.00424854,  0.00994768, -0.0082657 ,\n",
       "       -0.00750369,  0.00569355], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "w2v_model.wv['account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_dict = {}\n",
    "\n",
    "# for word in words:\n",
    "#     vector_dict[word] = w2v_model[]\n",
    "vector_dict = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.00454417, -0.00302309,  0.01508945, -0.01440222,  0.00749125,\n",
       "       -0.00686202,  0.01226617, -0.00274484, -0.0072758 , -0.00479306,\n",
       "       -0.01184115,  0.01131314,  0.00633893, -0.01220932,  0.01088379,\n",
       "        0.00815971,  0.01139953,  0.00808345,  0.01503804, -0.01106866,\n",
       "       -0.0108741 , -0.01006933, -0.00208098,  0.00145122,  0.00448123,\n",
       "       -0.00325523,  0.00813822,  0.00424854,  0.00994768, -0.0082657 ,\n",
       "       -0.00750369,  0.00569355], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "vector_dict['account']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a pickle file of the processed training data exists, then it will be loaded\n",
    "# Otherwise, the training data will be processed and saved in a pickle file\n",
    "\n",
    "if path.exists('./data.pickle'):\n",
    "    with open('data.pickle', 'rb') as file:\n",
    "        words, labels, training, output = pickle.load(file)\n",
    "else:\n",
    "    words, labels, training, output = preprocess_train_data(data)\n",
    "    with open('data.pickle', 'wb') as file:\n",
    "        pickle.dump((words, labels, training, output), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.44627\u001b[0m\u001b[0m | time: 0.056s\n",
      "| Adam | epoch: 200 | loss: 0.44627 - acc: 0.8385 -- iter: 56/60\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.43179\u001b[0m\u001b[0m | time: 0.064s\n",
      "| Adam | epoch: 200 | loss: 0.43179 - acc: 0.8547 -- iter: 60/60\n",
      "--\n",
      "INFO:tensorflow:c:\\Users\\owner\\Documents\\GitHub\\CustomerServiceBot-RW\\cs-bagofwords\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to create and train a new model\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(training, output, n_epoch=250, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load a previously trained model\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.load('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words, stemmer):\n",
    "    bag = list(np.zeros(len(words)))\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i]=1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear(): \n",
    "    # Uses os.system and os.name\n",
    "    # for windows \n",
    "    if name == 'nt': \n",
    "        _ = system('cls') \n",
    "    # for mac and linux \n",
    "    else: \n",
    "        _ = system('clear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "\n",
    "    clear()\n",
    "    greetings = [\"Hello! How can I help you today?\", \"Hello! What do you need help with today?\", \"Hi there, how can I help?\"]\n",
    "    print(choice(greetings))\n",
    "\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    def filter_punctuation(s):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        return regex.sub('', s)\n",
    "\n",
    "    while True:\n",
    "        prediction = None\n",
    "\n",
    "        inp = filter_punctuation(input(\"You: \"))\n",
    "        print(inp)\n",
    "\n",
    "        if inp.lower() in ['quit', 'exit', 'stop']:\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words, stemmer)])\n",
    "        result_index = np.argmax(results)\n",
    "        result_max = np.max(results)\n",
    "\n",
    "        if result_max > 0.66:\n",
    "\n",
    "            prediction = labels[result_index]\n",
    "            responses = data[prediction]['responses']\n",
    "            response = choice(responses)\n",
    "\n",
    "            # print(prediction)\n",
    "\n",
    "            print('Bot: ', choice(responses))\n",
    "                    \n",
    "        if prediction is 'goodbye':\n",
    "            break\n",
    "\n",
    "        elif prediction is None:\n",
    "            print(f\"Bot: I'm sorry, I didn't quite get that. Can you rephrase your question?\")\n",
    "\n",
    "        else:\n",
    "            print(choice())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello! What do you need help with today?\n",
      "how to open new account\n",
      "Bot:  To open a new account, you'll need proof of identification and an initial deposit of at least $100.00. Please call the customer service line to speak with a representative. You can also use our website to create one using the Account Creation tool or to find a location near you.\n",
      "what about closing an account\n",
      "Bot:  To open a new account, you'll need proof of identification and an initial deposit of at least $100.00. Please call the customer service line to speak with a representative. You can also use our website to create one using the Account Creation tool or to find a location near you.\n",
      "closing an account\n",
      "Bot:  To open a new account, you'll need proof of identification and an initial deposit of at least $100.00. Please call the customer service line to speak with a representative. You can also use our website to create one using the Account Creation tool or to find a location near you.\n",
      "i want to close my account\n",
      "Bot:  To close your account, you'll need to speak with a representative. Call the customer support line and have your account information ready. They will be able to send you a check or transfer money to another location.\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}