{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "c9c193319ebd2a89754c1a4aa4516d58b41ef9df354ad04f80777c68efefc770"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using transfer learning to create a chatbot\n",
    "This file can be used if the repo is implemented in cloud, such as in a Colab notebook.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't already, copy the repo to your current working environment and install requirements.txt\n",
    "!git clone https://github.com/rweddell/CustomerServiceBot-RW  \n",
    "!pip install -r /content/CustomerServiceBot-RW/cs-wordvectors/requirements.txt\n",
    "!python -m spacy download en "
   ]
  },
  {
   "source": [
    "### Train the model\n",
    "The train.py script imports a pretrained BERT model and trains it on a given dataset or the default dataset in Hugging Face's S3 bucket.   \n",
    "With the parameters as shown, the training should take about 30 minutes using a GPU.  \n",
    "\n",
    "Argument | Type | Default value | Description\n",
    "---------|------|---------------|------------\n",
    "dataset_path | `str` | `\"\"` | Path or url of the dataset. If empty download from S3.\n",
    "dataset_cache | `str` | `'./dataset_cache.bin'` | Path or url of the dataset cache\n",
    "model | `str` | `\"openai-gpt\"` | Path, url or short name of the model\n",
    "num_candidates | `int` | `2` | Number of candidates for training\n",
    "max_history | `int` | `2` | Number of previous exchanges to keep in history\n",
    "train_batch_size | `int` | `4` | Batch size for training\n",
    "valid_batch_size | `int` | `4` | Batch size for validation\n",
    "gradient_accumulation_steps | `int` | `8` | Accumulate gradients on several steps\n",
    "lr | `float` | `6.25e-5` | Learning rate\n",
    "lm_coef | `float` | `1.0` | LM loss coefficient\n",
    "mc_coef | `float` | `1.0` | Multiple-choice loss coefficient\n",
    "max_norm | `float` | `1.0` | Clipping gradient norm\n",
    "n_epochs | `int` | `3` | Number of training epochs\n",
    "personality_permutations | `int` | `1` | Number of permutations of personality sentences\n",
    "device | `str` | `\"cuda\" if torch.cuda.is_available() else \"cpu\"` | Device (cuda or cpu)\n",
    "fp16 | `str` | `\"\"` | Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\n",
    "local_rank | `int` | `-1` | Local rank for distributed training (-1: not distributed)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/CustomerServiceBot-RW/cs-wordvectors/hugging-face/train.py --dataset_path=\"/content/CustomerServiceBot-RW/cs-wordvectors/cs_training_data.json\" --n_epochs=3 --train_batch_size=3 --valid_batch_size=3 --max_history=4  "
   ]
  },
  {
   "source": [
    "Chat with the trained model using interact.py.   \n",
    "If a model_checkpoint is not specified, interact.py will reference a previously trained model from Hugging Face's S3 bucket.  \n",
    "\n",
    "Argument | Type | Default value | Description\n",
    "---------|------|---------------|------------\n",
    "dataset_path | `str` | `\"\"` | Path or url of the dataset. If empty download from S3.\n",
    "dataset_cache | `str` | `'./dataset_cache.bin'` | Path or url of the dataset cache\n",
    "model | `str` | `\"openai-gpt\"` | Path, url or short name of the model\n",
    "max_history | `int` | `2` | Number of previous utterances to keep in history\n",
    "device | `str` | `cuda` if `torch.cuda.is_available()` else `cpu` | Device (cuda or cpu)\n",
    "no_sample | action `store_true` | Set to use greedy decoding instead of sampling\n",
    "max_length | `int` | `20` | Maximum length of the output utterances\n",
    "min_length | `int` | `1` | Minimum length of the output utterances\n",
    "seed | `int` | `42` | Seed\n",
    "temperature | `int` | `0.7` | Sampling softmax temperature\n",
    "top_k | `int` | `0` | Filter top-k tokens before sampling (`<=0`: no filtering)\n",
    "top_p | `float` | `0.9` | Nucleus filtering (top-p) before sampling (`<=0.0`: no filtering)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example location from a Colab run\n",
    "#!python ./hugging-face/interact.py --model_checkpoint '/content/runs/Apr03_19-59-03_b8f756943518_openai-gpt'\n",
    "\n",
    "!python ./hugging-face/interact.py --model_checkpoint='<path/to/trained/model/>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}