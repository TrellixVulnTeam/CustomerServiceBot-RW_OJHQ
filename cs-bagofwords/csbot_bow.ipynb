{
 "cells": [
  {
   "source": [
    "This notebook creates, trains, and initializes a chatbot using the bag-of-words model. \n",
    "\n",
    "\n",
    "https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import path, name, system\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from random import choice, randint\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import nltk, pickle, json, re, string, tflearn, spacy, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cs_prompts.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and format the training data\n",
    "def preprocess_train_data(data):\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    words = []\n",
    "    labels = list(data.keys())\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for label in labels:\n",
    "        for pattern in data[label]['patterns']:\n",
    "            tokens = nltk.word_tokenize(pattern)\n",
    "            words.extend(tokens)\n",
    "            docs_x.append(tokens)\n",
    "            docs_y.append(label)\n",
    "\n",
    "    # Pass over punctuation tokens\n",
    "    ignored_tokens = [',', '.', '?', '!']\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignored_tokens]\n",
    "\n",
    "    words = sorted(set(words))\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    # Template for the BOW\n",
    "    out_empty = list(np.zeros(len(labels)))\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "        stemmed = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in stemmed:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)  \n",
    "\n",
    "    training = np.array(training)\n",
    "    output = np.array(output)    \n",
    "    \n",
    "    return words, labels, training, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a pickle file of the processed training data exists, then it will be loaded\n",
    "# Otherwise, the training data will be processed and saved in a pickle file\n",
    "\n",
    "if path.exists('./data.pickle'):\n",
    "    with open('data.pickle', 'rb') as file:\n",
    "        words, labels, training, output = pickle.load(file)\n",
    "else:\n",
    "    words, labels, training, output = preprocess_train_data(data)\n",
    "    with open('data.pickle', 'wb') as file:\n",
    "        pickle.dump((words, labels, training, output), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.00418\u001b[0m\u001b[0m | time: 0.059s\n",
      "| Adam | epoch: 300 | loss: 0.00418 - acc: 1.0000 -- iter: 56/60\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.00427\u001b[0m\u001b[0m | time: 0.068s\n",
      "| Adam | epoch: 300 | loss: 0.00427 - acc: 1.0000 -- iter: 60/60\n",
      "--\n",
      "INFO:tensorflow:c:\\Users\\owner\\Documents\\GitHub\\CustomerServiceBot-RW\\cs-bagofwords\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to create and train a new model\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(training, output, n_epoch=250, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\owner\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tflearn\\initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Restoring parameters from c:\\Users\\owner\\Documents\\GitHub\\CustomerServiceBot-RW\\cs-bagofwords\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to load a previously trained model\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "model.load('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear(): \n",
    "    # Uses os.system and os.name\n",
    "    # for windows \n",
    "    if name == 'nt': \n",
    "        _ = system('cls') \n",
    "    # for mac/linux \n",
    "    else: \n",
    "        _ = system('clear') \n",
    "\n",
    "def bag_of_words(s, words, stemmer):\n",
    "    # Creates a bag of words from a given sequence of tokens\n",
    "    bag = list(np.zeros(len(words)))\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i]=1\n",
    "    return np.array(bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_chat():\n",
    "    history = None\n",
    "    with open(\"history.json\") as h_file:\n",
    "        history = json.load(h_file)\n",
    "    clear()\n",
    "    print(choice(data['greeting']['responses']))\n",
    "\n",
    "    stemmer = LancasterStemmer()\n",
    "    ner = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def filter_punctuation(s):\n",
    "        # Uses regular expressions to filter non-alphabetical characters from strings\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        return regex.sub('', s)\n",
    "\n",
    "    def classify(user_input):\n",
    "        results = model.predict([bag_of_words(user_input, words, stemmer)])\n",
    "        result_index=np.argmax(results)\n",
    "        result_max = np.max(results)\n",
    "        prediction = labels[result_index]\n",
    "        responses = data[prediction]['responses']\n",
    "        response = choice(responses)\n",
    "        return prediction, response\n",
    "\n",
    "    CHAT_ENDED = False\n",
    "\n",
    "    def end_chat(inp):\n",
    "        # print(inp)\n",
    "        if inp.lower() in ['end', 'quit', 'stop']:\n",
    "            CHAT_ENDED = True\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    while not CHAT_ENDED:\n",
    "        prediction = None\n",
    "        inp = filter_punctuation(input(\"You: \").lower())\n",
    "\n",
    "        if CHAT_ENDED:\n",
    "            break\n",
    "\n",
    "        prediction, response = classify(inp)\n",
    "\n",
    "        print(f\"Bot: {response}\")\n",
    "        \n",
    "        if prediction in ['open_account', 'close_account']:\n",
    "            user_name = []\n",
    "            user_account_num = None\n",
    "            while len(user_name) < 1:\n",
    "                print(\"Bot: Please give me your first and last name\")\n",
    "                inp = input(\"You: \")\n",
    "\n",
    "                if end_chat(inp):\n",
    "                    break\n",
    "\n",
    "                parts = ner(inp)\n",
    "                for part in parts:\n",
    "                    if part.pos_ == 'PROPN':\n",
    "                        user_name.append(part.text)\n",
    "                if len(user_name) > 0:\n",
    "                    print(f\"Bot: Your name is {' '.join(user_name)}. Is that correct?\")\n",
    "                    inp = input(\"You: \")\n",
    "\n",
    "                    if end_chat(inp):\n",
    "                        break\n",
    "\n",
    "                    name_prediction, response = classify(filter_punctuation(inp.lower()))\n",
    "                    \n",
    "                    if name_prediction == 'deny':\n",
    "                        print(\"Bot: Sorry about that. Let me try again.\")\n",
    "                        user_name = []\n",
    "                        user_account_num = None\n",
    "                        continue\n",
    "                    \n",
    "                    elif name_prediction == 'confirm':\n",
    "                        \n",
    "                        if prediction == 'open_account':\n",
    "                            user_account_num = ''.join([str(randint(1,9)) for i in range(12)])\n",
    "                            print(f\"Your new account number is {user_account_num}\")\n",
    "                            history[' '.join(user_name)] = user_account_num\n",
    "                        \n",
    "                        elif prediction == 'close_account':\n",
    "                            print(\"Bot: Can you give me your account number?\")\n",
    "                            inp = input(\"You: \")\n",
    "\n",
    "                            if end_chat(inp):\n",
    "                                break\n",
    "\n",
    "                            parts = ner(inp)\n",
    "                            for part in parts:\n",
    "                                if part.pos_ == 'NUM':\n",
    "                                    user_account_num = part.text\n",
    "                            print(f\"Bot: Your account number is {user_account_num}. Is that correct?\")\n",
    "                            inp = input(\"You: \")\n",
    "\n",
    "                            if end_chat(inp):\n",
    "                                break\n",
    "                            \n",
    "                            confirm_prediction, confirm_response = classify(filter_punctuation(inp.lower()))\n",
    "                            \n",
    "                            if prediction == 'confirm':\n",
    "                                print(f\"Bot: {response}\")\n",
    "                                history[\" \".join(user_name)] = None\n",
    "                            elif prediction == 'deny':\n",
    "                                print(\"Bot: Sorry about that. Let me try again.\")\n",
    "\n",
    "        if prediction == 'goodbye':\n",
    "            break\n",
    "\n",
    "        elif prediction == None:\n",
    "        #    print(f\"Bot: I'm sorry, I didn't quite get that. Can you rephrase?\")\n",
    "            print(f\"Bot: {choice(data['deny']['responses'])}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Bot: Is there anything else I can help with?\")\n",
    "\n",
    "    with open(\"history.json\", \"w\") as h_file:\n",
    "        json.dump(history, h_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hi there, how can I help?\n",
      "Bot: To open a new account, you'll need proof of identification and an initial deposit of at least $100.00. \n",
      "Please call the customer service line to speak with a representative. \n",
      "You can also use our website to create one using the Account Creation tool or to find a location near you.\n",
      "Bot: Please give me your first and last name\n",
      "Bot: Your name is Rob Weddell. Is that correct?\n",
      "Your new account number is 332483333371\n",
      "Bot: Is there anything else I can help with?\n",
      "Bot: Great. Thanks for confirming.\n",
      "Bot: Is there anything else I can help with?\n",
      "Bot: Account management can be done through BotBank's website, \n",
      "or if you need assistance, \n",
      "you can call the customer support line to speak with a representative.\n",
      "Bot: Is there anything else I can help with?\n",
      "Bot: Glad I could help. Thanks for choosing BotBank Have a nice day.\n"
     ]
    }
   ],
   "source": [
    "context_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}